{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6977fb",
   "metadata": {
    "id": "fb6977fb"
   },
   "source": [
    "## 1.1 Cài đặt thư viện\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31349cf",
   "metadata": {
    "id": "c31349cf"
   },
   "source": [
    "## 1.2 Thêm các thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3c40b",
   "metadata": {
    "id": "cde3c40b"
   },
   "outputs": [],
   "source": [
    "#import standar dependentcies\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d5d9e",
   "metadata": {
    "id": "260d5d9e"
   },
   "outputs": [],
   "source": [
    "#import tansorflow dependentcies\n",
    "# Import tensorflow dependencies - Functional API\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import random\n",
    "import tensorflow.keras.backend as k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BGJGrRQoRY5P",
   "metadata": {
    "id": "BGJGrRQoRY5P"
   },
   "source": [
    "## 1.3 Set GPU Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PEyyYI7DRU6S",
   "metadata": {
    "id": "PEyyYI7DRU6S"
   },
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b905b",
   "metadata": {
    "id": "7a1b905b"
   },
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "\n",
    "    # Read in image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    # Load in the image\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "\n",
    "    # Preprocessing steps - resizing the image to be 100x100x3\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    # Scale image to be between 0 and 1\n",
    "    img = img / 255.0\n",
    "\n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a23a2",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74619ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lfw_dataset(data_dir):\n",
    "    # Load the LFW dataset from the specified directory\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for person_dir in os.listdir(data_dir):\n",
    "        person_path = os.path.join(data_dir, person_dir)\n",
    "        \n",
    "        if not os.path.isdir(person_path):\n",
    "            continue\n",
    "        \n",
    "        for image_name in os.listdir(person_path):\n",
    "            image_path = os.path.join(person_path, image_name)\n",
    "            \n",
    "            # Read in image from file path\n",
    "            byte_img = tf.io.read_file(image_path)\n",
    "            # Load in the image \n",
    "            img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "            # Preprocessing steps - resizing the image to be 100x100x3\n",
    "            img = tf.image.resize(img, (100,100))\n",
    "            # Scale image to be between 0 and 1 \n",
    "            img = img / 255.0\n",
    "            \n",
    "            images.append(img)\n",
    "            # Assign a unique label to each person\n",
    "            person_label = person_dir\n",
    "            labels.append(person_label)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d179fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_pairs(images, labels, target_size):\n",
    "    pairs_anchor = []\n",
    "    pairs_val = []\n",
    "    target = []\n",
    "    \n",
    "    # Create positive pairs (same person)\n",
    "    for i in range(len(images)-(target_size + 1)):\n",
    "        for j in range(i+1, i + (target_size + 1)):\n",
    "            if labels[i] == labels[j]:\n",
    "                pairs_anchor.append((images[i]))\n",
    "                pairs_val.append((images[j]))\n",
    "                target.append(1)\n",
    "#         print('lable =  1')\n",
    "    # Create negative pairs (different persons)\n",
    "    for i in range(len(images)-(target_size + 1)):\n",
    "        for j in range(i+1, i + (target_size + 1)):\n",
    "            if labels[i] != labels[j]:\n",
    "                pairs_anchor.append((images[i]))\n",
    "                pairs_val.append((images[j]))\n",
    "                target.append(0)\n",
    "#         print('lable =  0')\n",
    "    \n",
    "    return pairs_anchor, pairs_val, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lPZ314lzhOZN",
   "metadata": {
    "id": "lPZ314lzhOZN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs_anchor = np.load('siamese_dataset_pairs_anchor.npy')\n",
    "pairs_val = np.load('siamese_dataset_pairs_val.npy')\n",
    "labels_dataset = np.load('siamese_dataset_target.npy')\n",
    "\n",
    "pairs_anchor = np.array(pairs_anchor[:600])\n",
    "pairs_val = np.array(pairs_val[:600])\n",
    "labels_dataset = np.array(labels_dataset[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_dataset = tf.convert_to_tensor(pairs_anchor)\n",
    "pairs_dataset = tf.convert_to_tensor(pairs_val)\n",
    "labels_dataset = tf.convert_to_tensor(labels_dataset)\n",
    "\n",
    "# Giải phóng biến sau khi sử dụng\n",
    "del pairs_anchor, pairs_val\n",
    "\n",
    "# Tạo tập dữ liệu 2 lớp được gán nhãn\n",
    "data = tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(anchor_dataset),\n",
    "    tf.data.Dataset.from_tensor_slices(pairs_dataset),\n",
    "    tf.data.Dataset.from_tensor_slices(labels_dataset)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c750151",
   "metadata": {},
   "outputs": [],
   "source": [
    "del anchor_dataset, pairs_dataset,labels_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2859e9",
   "metadata": {
    "id": "7b2859e9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca61235",
   "metadata": {
    "id": "bca61235"
   },
   "outputs": [],
   "source": [
    "# Build dataloader pipeline\n",
    "data = data.cache()\n",
    "# trộn dữ liệu, chỉ định bộ đệm 1024\n",
    "data = data.shuffle(buffer_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020cf390",
   "metadata": {
    "id": "020cf390"
   },
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data)*.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a898c",
   "metadata": {
    "id": "b00a898c"
   },
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "test_data = data.skip(round(len(data)*.7))  # Bỏ qua data train\n",
    "test_data = test_data.take(round(len(data)*.3))  # Lấy 30% cuối cùng\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)\n",
    "\n",
    "test_input, test_val, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ad167",
   "metadata": {
    "id": "053ad167"
   },
   "source": [
    "# Build embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b1f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding(): \n",
    "    inp = Input(shape=(100,100,3), name='input_image')\n",
    "    \n",
    "    # First block\n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "    \n",
    "    # Second block\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    # Third block \n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "    \n",
    "    # Final embedding block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "    \n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac510f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9daacb1",
   "metadata": {},
   "source": [
    "## 4.2 Build Distance Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41258a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese L1 Distance class\n",
    "class L1Dist(Layer):\n",
    "    \n",
    "    # Init method - inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "       \n",
    "    # Euclidean distance calculation\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = L1Dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340b32c",
   "metadata": {},
   "source": [
    "## 4.3 Make Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model(): \n",
    "    \n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_img', shape=(100,100,3))\n",
    "    \n",
    "    # Validation image in the network \n",
    "    validation_image = Input(name='validation_img', shape=(100,100,3))\n",
    "    \n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    # Classification layer \n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aad834",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee04ef",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d46e8",
   "metadata": {
    "id": "866d46e8"
   },
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(a,b,c):\n",
    "    for idx, batch in enumerate(test_data):\n",
    "        y_hat = siamese_model.predict([batch[0], batch[1]])\n",
    "        y_true = batch[2]\n",
    "        y_hat_round = []\n",
    "        for prediction in y_hat:\n",
    "            if prediction > 0.9:\n",
    "                y_hat_round.append(1)\n",
    "            else:\n",
    "                y_hat_round.append(0)\n",
    "        true = 0\n",
    "\n",
    "        for idx in range(0, len(y_hat_round)-1):\n",
    "            if y_hat_round[idx] == y_true[idx]:\n",
    "                true = true + 1\n",
    "            # print(true)\n",
    "        correct_ratio = true/len(y_hat)\n",
    "    print('val_accuracy',correct_ratio)\n",
    "    \n",
    "    return correct_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be5cb4",
   "metadata": {
    "id": "d6be5cb4"
   },
   "source": [
    "## 5.1 Setup Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80f50f8",
   "metadata": {
    "id": "c80f50f8"
   },
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.Adam(1e-4) # 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88c1ff",
   "metadata": {
    "id": "0e88c1ff"
   },
   "source": [
    "## 5.2 Establish Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3354ca",
   "metadata": {
    "id": "3f3354ca"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2be3e",
   "metadata": {
    "id": "2ca2be3e"
   },
   "source": [
    "## 5.3 Build Train Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe94f62",
   "metadata": {
    "id": "ffe94f62",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Record all of our operations \n",
    "    with tf.GradientTape() as tape:     \n",
    "        # Get anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "        \n",
    "        # Forward pass\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        # Calculate loss\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "        \n",
    "    print(loss)\n",
    "        \n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    # Calculate updated weights and apply to siamese model\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "        \n",
    "    # Return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c0513",
   "metadata": {
    "id": "d08c0513"
   },
   "source": [
    "## 5.4 Build Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c5c8c",
   "metadata": {
    "id": "749c5c8c"
   },
   "outputs": [],
   "source": [
    "# Import metric calculations\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8004aa",
   "metadata": {
    "id": "5f8004aa"
   },
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "\n",
    "    loss_numpys = []\n",
    "    r_results = []\n",
    "    p_results = []\n",
    "    val_accuracy = []\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Creating a metric object \n",
    "        r = Recall()\n",
    "        p = Precision()\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            loss = train_step(batch)\n",
    "            yhat = siamese_model.predict(batch[:2])\n",
    "            r.update_state(batch[2], yhat)\n",
    "            p.update_state(batch[2], yhat) \n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        print(loss.numpy(), r.result().numpy(), p.result().numpy())\n",
    "        \n",
    "        loss_numpys.append(loss.numpy())\n",
    "        r_results.append(r.result().numpy())\n",
    "        p_results.append(p.result().numpy())\n",
    "        \n",
    "        accuracy_calculation = calculate_accuracy(test_input, test_val, y_true)\n",
    "        val_accuracy.append(accuracy_calculation)\n",
    "        # Save checkpoints\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    # return val_accuracy\n",
    "    return (loss_numpys, r_results, p_results, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb16d4",
   "metadata": {
    "id": "96fb16d4"
   },
   "source": [
    "## 5.5 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676bb2a1",
   "metadata": {
    "id": "676bb2a1"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TTSwRRbgVWB6",
   "metadata": {
    "id": "TTSwRRbgVWB6"
   },
   "outputs": [],
   "source": [
    "siamese_model.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46008259",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46008259",
    "outputId": "8bf7cd19-b0f5-4416-942e-f45348cfe43f",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_numpy, r_result, p_result, val_accuracy = train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hYh0987YzcCo",
   "metadata": {
    "id": "hYh0987YzcCo"
   },
   "outputs": [],
   "source": [
    "# create x-axis values\n",
    "x = range(len(val_accuracy))\n",
    "# loss.numpy(), r.result().numpy(), p.result().numpy(), val_accuracy\n",
    "# plot the data\n",
    "plt.plot(x, loss_numpy, label='loss')\n",
    "plt.plot(x, r_result, label='r.result')\n",
    "plt.plot(x, p_result, label='p.result')\n",
    "plt.plot(x, val_accuracy, label='val_accuracy')\n",
    "\n",
    "# Đánh dấu điểm có giá trị lớn nhất\n",
    "max_index = val_accuracy.index(max(val_accuracy))\n",
    "plt.scatter(x[max_index], val_accuracy[max_index], color='red', label='Max Value')\n",
    "\n",
    "plt.annotate(f'({x[max_index]}, {val_accuracy[max_index]})',\n",
    "             xy=(x[max_index], val_accuracy[max_index]), xytext=(x[max_index]+0.3, val_accuracy[max_index]-0.2),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "\n",
    "# Đánh dấu điểm có giá trị nhỏ nhất\n",
    "min_index = loss_numpy.index(min(loss_numpy))\n",
    "plt.scatter(x[min_index], loss_numpy[min_index], color='red', label='Min Value')\n",
    "\n",
    "plt.annotate(f'({x[min_index]}, {loss_numpy[min_index]})',\n",
    "             xy=(x[min_index], loss_numpy[min_index]), xytext=(x[min_index]+0.3, loss_numpy[min_index]+0.2),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "\n",
    "\n",
    "\n",
    "# add labels\n",
    "plt.title('Facial Verification using LFW')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Hiển thị bảng chú thích\n",
    "legend = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), bbox_transform=plt.gcf().transFigure)\n",
    "\n",
    "\n",
    "# plt.grid(True)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, color='gray')\n",
    "\n",
    "# save image\n",
    "plt.savefig('Facial Verification using LFW(kha_quan)(600)(50).png', dpi=500, bbox_inches='tight')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800465ab",
   "metadata": {
    "id": "800465ab",
    "tags": []
   },
   "source": [
    "# 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3cea47",
   "metadata": {
    "id": "9d3cea47"
   },
   "source": [
    "## 6.1 Import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9edb15c",
   "metadata": {
    "id": "d9edb15c"
   },
   "outputs": [],
   "source": [
    "# Import metric calculations\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763768b7",
   "metadata": {
    "id": "763768b7"
   },
   "source": [
    "## 6.2 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92de35b",
   "metadata": {
    "id": "a92de35b"
   },
   "outputs": [],
   "source": [
    "# Get a batch of test data\n",
    "test_input, test_val, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368b83f",
   "metadata": {
    "id": "e368b83f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_hat = siamese_model.predict([test_input, test_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f8c67",
   "metadata": {
    "id": "0c8f8c67",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Post processing the results\n",
    "[1 if prediction > 0.8 else 0 for prediction in y_hat ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db65eb91",
   "metadata": {
    "id": "db65eb91",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b520a",
   "metadata": {
    "id": "ca5b520a"
   },
   "source": [
    "## 6.3 Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bceacf2",
   "metadata": {
    "id": "6bceacf2"
   },
   "outputs": [],
   "source": [
    "# Creating a metric object\n",
    "m = Recall()\n",
    "\n",
    "# Calculating the recall value\n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Return Recall Result\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2beb8f",
   "metadata": {
    "id": "1e2beb8f"
   },
   "outputs": [],
   "source": [
    "# Creating a metric object\n",
    "m = Precision()\n",
    "\n",
    "# Calculating the recall value\n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Return Recall Result\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb9f74",
   "metadata": {
    "id": "d0bb9f74"
   },
   "outputs": [],
   "source": [
    "r = Recall()\n",
    "p = Precision()\n",
    "\n",
    "for test_input, test_val, y_true in test_data.as_numpy_iterator():\n",
    "    yhat = siamese_model.predict([test_input, test_val])\n",
    "    r.update_state(y_true, yhat)\n",
    "    p.update_state(y_true,yhat)\n",
    "\n",
    "print(r.result().numpy(), p.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iFGyyBZV1rS5",
   "metadata": {
    "id": "iFGyyBZV1rS5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_resul = 0\n",
    "for idx in range(0,100):\n",
    "\n",
    "    test_input, test_val, y_true = test_data.as_numpy_iterator().next()\n",
    "\n",
    "    y_hat = siamese_model.predict([test_input, test_val])\n",
    "    y_hat_round = []\n",
    "    for prediction in y_hat:\n",
    "        if prediction > 0.9:\n",
    "            y_hat_round.append(1)\n",
    "        else:\n",
    "            y_hat_round.append(0)\n",
    "    true = 0\n",
    "    for idx in range(len(y_true)):\n",
    "        if y_hat_round[idx] == y_true[idx]:\n",
    "            true = true + 1\n",
    "    print(true)\n",
    "    test_resul = test_resul + true\n",
    "\n",
    "correct_ratio = test_resul/len(y_hat)\n",
    "print(correct_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5uJLIL-sA0Ym",
   "metadata": {
    "id": "5uJLIL-sA0Ym"
   },
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "THvupu7fA3Pq",
   "metadata": {
    "id": "THvupu7fA3Pq"
   },
   "outputs": [],
   "source": [
    "test_input = np.load('test_siamese_dataset_pairs_anchor.npy')\n",
    "test_val = np.load('test_siamese_dataset_pairs_val.npy')\n",
    "y_true = np.load('test_siamese_dataset_target.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cCtjQx0fO4G1",
   "metadata": {
    "id": "cCtjQx0fO4G1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_hat = siamese_model.predict([test_input, test_val])\n",
    "y_hat_round = []\n",
    "for prediction in y_hat:\n",
    "    if prediction > 0.9:\n",
    "        y_hat_round.append(1)\n",
    "    else:\n",
    "        y_hat_round.append(0)\n",
    "true = 0\n",
    "\n",
    "for idx in range(0, len(y_hat_round)-1):\n",
    "    if y_hat_round[idx] == y_true[idx]:\n",
    "        true = true + 1\n",
    "#     print(true)\n",
    "\n",
    "correct_ratio = true/len(y_hat)\n",
    "print(correct_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce525bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.load('LFW_pairs_anchor.npy')\n",
    "test_val = np.load('LFW_pairs_val.npy')\n",
    "y_true = np.load('LFW_target.npy')\n",
    "\n",
    "y_hat = siamese_model.predict([test_input, test_val])\n",
    "y_hat_round = []\n",
    "for prediction in y_hat:\n",
    "    if prediction > 0.95:\n",
    "        y_hat_round.append(1)\n",
    "    else:\n",
    "        y_hat_round.append(0)\n",
    "true = 0\n",
    "\n",
    "for idx in range(0, len(y_hat_round)-1):\n",
    "    if y_hat_round[idx] == y_true[idx]:\n",
    "        true = true + 1\n",
    "  # print(true)\n",
    "correct_ratio = true/len(y_hat)\n",
    "print(correct_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1c8eb",
   "metadata": {
    "id": "d5e1c8eb"
   },
   "source": [
    "## 6.4 Viz Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb0f45",
   "metadata": {
    "id": "11fb0f45",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set plot size\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Set first subplot\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_input[0])\n",
    "\n",
    "# Set second subplot\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[0])\n",
    "\n",
    "# Renders cleanly\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be203a57",
   "metadata": {
    "id": "be203a57"
   },
   "source": [
    "# 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8cedc",
   "metadata": {
    "id": "f6a8cedc"
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "siamese_model.save('siamesemodel_LFW(600)(50).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7926e12",
   "metadata": {
    "id": "a7926e12"
   },
   "outputs": [],
   "source": [
    "# # Reload model\n",
    "# model = tf.keras.models.load_model('siamesemodel_LFW_v1_0.h5',\n",
    "#                                    custom_objects={'L1Dist':L1Dist, 'contrastive_loss':contrastive_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df332b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model \n",
    "model = tf.keras.models.load_model('siamesemodel_LFW(600)(50).h5', \n",
    "                                   custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
